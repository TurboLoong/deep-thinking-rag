{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "toc-pro-adv",
   "metadata": {},
   "source": [
    "# A Guide to Production-Grade RAG: From Theory to Autonomous Agents\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**Part 1: Setting the Stage - Foundations and Our Core Challenge**\n",
    "* [1.1. Introduction: The Limits of \"Shallow\" RAG](#part1-1-intro-pro)\n",
    "* [1.2. Environment Setup: API Keys, Imports, and Configuration](#part1-2-env-pro-adv)\n",
    "* [1.3. The Dataset: Preparing Our Knowledge Base](#part1-3-data-pro)\n",
    "* [1.4. The Upgraded Challenge: A Multi-Source, Multi-Hop Query](#part1-4-challenge-pro-adv)\n",
    "\n",
    "**Part 2: The Baseline - Building and Breaking a \"Vanilla\" RAG Pipeline**\n",
    "* [2.1. Code Dependency: Document Loading and Naive Chunking](#part2-1-dep-pro)\n",
    "* [2.2. Code Dependency: Creating the Vector Store](#part2-2-dep-pro)\n",
    "* [2.3. Code Dependency: Assembling the Simple RAG Chain](#part2-3-dep-pro)\n",
    "* [2.4. The Critical Failure Case: Demonstrating the Need for Advanced Techniques](#part2-4-fail-pro-adv)\n",
    "* [2.5. Diagnosis: Why Did It Fail?](#part2-5-diag-pro-adv)\n",
    "\n",
    "**Part 3: The \"Deep Thinking\" Upgrade: Engineering an Autonomous Reasoning Engine**\n",
    "* [3.1. Code Dependency: Defining the `RAGState`](#part3-1-state-pro-adv)\n",
    "* [3.2. Component 1: Dynamic Planning and Query Formulation](#part3-2-planner-pro-adv)\n",
    "    * [3.2.1. The Tool-Aware Planner Agent](#part3-2-1-planner-pro-adv)\n",
    "    * [3.2.2. Query Rewriting and Expansion](#part3-2-2-rewriter-pro)\n",
    "    * [3.2.3. Entity and Constraint Extraction](#part3-2-3-metadata-pro)\n",
    "* [3.3. Component 2: The Multi-Stage, Adaptive Retrieval Funnel](#part3-3-retrieval-pro-adv)\n",
    "    * [3.3.1. NEW: The Retrieval Supervisor Agent](#part3-3-1-supervisor-pro)\n",
    "    * [3.3.2. Implementing the Retrieval Strategies](#part3-3-2-strategies-pro)\n",
    "    * [3.3.3. Stage 2 (High Precision): Cross-Encoder Reranker](#part3-3-3-reranker-pro)\n",
    "    * [3.3.4. Stage 3 (Contextual Distillation)](#part3-3-4-distill-pro)\n",
    "* [3.4. Component 3: Tool Augmentation with Web Search](#part3-4-tool-pro)\n",
    "* [3.5. Component 4: The Self-Critique and Control Flow Policy](#part3-5-critique-pro)\n",
    "    * [3.5.1. The \"Update and Reflect\" Step](#part3-5-1-reflect-pro)\n",
    "    * [3.5.2. Policy Implementation (LLM-as-a-Judge)](#part3-5-2-policy-pro)\n",
    "    * [3.5.3. Defining Robust Stopping Criteria](#part3-5-3-stopping-pro)\n",
    "\n",
    "**Part 4: Assembly with LangGraph - Orchestrating the Reasoning Loop**\n",
    "* [4.1. Code Dependency: Defining the Graph Nodes](#part4-1-nodes-pro-adv)\n",
    "* [4.2. Code Dependency: Defining the Conditional Edges](#part4-2-edges-pro-adv)\n",
    "* [4.3. Building the `StateGraph`](#part4-3-build-pro-adv)\n",
    "* [4.4. Compiling and Visualizing the Workflow](#part4-4-viz-pro-adv)\n",
    "\n",
    "**Part 5: Redemption - Running the Advanced Agent**\n",
    "* [5.1. Invoking the Graph: A Step-by-Step Trace](#part5-1-invoke-pro-adv)\n",
    "* [5.2. Analyzing the Final High-Quality Output](#part5-2-analyze-pro-adv)\n",
    "* [5.3. Side-by-Side Comparison: Vanilla vs. Deep Thinking RAG](#part5-3-compare-pro-adv)\n",
    "\n",
    "**Part 6: A Production-Grade Evaluation Framework**\n",
    "* [6.1. Evaluation Metrics Overview](#part6-metrics-pro)\n",
    "* [6.2. Code Dependency: Implementing Evaluation with RAGAs](#part6-4-ragas-code-pro-adv)\n",
    "* [6.3. Interpreting the Evaluation Scores](#part6-5-interpret-pro-adv)\n",
    "\n",
    "**Part 7: Optimizations and Production Considerations**\n",
    "* [7.1. Optimization: Caching](#part7-1-cache-pro)\n",
    "* [7.2. Feature: Provenance and Citations](#part7-2-provenance-pro)\n",
    "* [7.3. Discussion: The Next Level - MDPs and Learned Policies](#part7-3-discussion-pro)\n",
    "* [7.4. Handling Failure: Graceful Exits and Fallbacks](#part7-4-failure-pro)\n",
    "\n",
    "**Part 8: Conclusion and Key Takeaways**\n",
    "* [8.1. Summary of Our Journey](#part8-conclusion-pro)\n",
    "* [8.2. Key Architectural Principles of Advanced RAG Systems](#part8-2-principles-pro-adv)\n",
    "* [8.3. Future Directions](#part8-3-future-pro-adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-intro-rag-pro",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Stage - Foundations and Our Core Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-1-intro-pro",
   "metadata": {},
   "source": [
    "### 1.1. Introduction: The Limits of \"Shallow\" RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has become the dominant paradigm for creating knowledge-intensive AI systems. The standard approach—a linear, three-step pipeline of **Retrieve -> Augment -> Generate**—is remarkably effective for simple, fact-based queries. However, this \"shallow\" RAG architecture reveals critical weaknesses when faced with complex questions that demand synthesis, comparison, and multi-step reasoning across a large and varied knowledge base.\n",
    "\n",
    "The next frontier in RAG is not about bigger models or larger context windows, but about greater **autonomy and intelligence** in the retrieval and reasoning process. The industry is moving from static chains to dynamic, agentic systems that can emulate a human researcher's workflow. These systems can decompose complex problems, select appropriate tools, dynamically adapt their retrieval strategies, and critique their own progress.\n",
    "\n",
    "In this comprehensive guide, we will build a powerful, **standalone** implementation of a **Deep Thinking RAG Pipeline**. We will meticulously engineer every component, from a sophisticated multi-stage, adaptive retrieval funnel to a tool-augmented, self-critiquing policy engine. We will begin by exposing the failure of a vanilla RAG system on a challenging query, and then, step-by-step, construct our advanced agent using **LangGraph** to orchestrate its complex, cyclical reasoning. By the end, you will have a production-grade framework and a deep, architectural understanding of how to build RAG systems that can truly *think*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-2-env-pro-adv",
   "metadata": {},
   "source": [
    "### 1.2. Environment Setup: API Keys, Imports, and Configuration\n",
    "\n",
    "We begin by setting up our foundational components. This includes securely managing API keys, importing all necessary libraries, and defining a global configuration dictionary. We will use **LangSmith** for tracing, which is an indispensable tool for visualizing and debugging the complex, non-linear execution paths of our reasoning agent. For our new web search capability, we will also add the **Tavily AI** API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part1-2-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and configuration set up successfully.\n",
      "{'data_dir': './data',\n",
      " 'vector_store_dir': './vector_store',\n",
      " 'llm_provider': 'openai',\n",
      " 'reasoning_llm': 'gpt-4o',\n",
      " 'fast_llm': 'gpt-4o-mini',\n",
      " 'embedding_model': 'text-embedding-3-small',\n",
      " 'reranker_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
      " 'max_reasoning_iterations': 7,\n",
      " 'top_k_retrieval': 10,\n",
      " 'top_n_rerank': 3}"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain langgraph langchain_openai chromadb beautifulsoup4 rank_bm25 lxml sentence-transformers cross-encoder ragas arxiv rich sec-api unstructured[html] tavily-python\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "from typing import List, Dict, TypedDict, Literal, Optional\n",
    "\n",
    "# Securely set API keys\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Enter your {var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "# Optional: For accessing SEC filings programmatically\n",
    "# _set_env(\"SEC_API_KEY\")\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Advanced-Deep-Thinking-RAG-v2\"\n",
    "\n",
    "# Central Configuration Dictionary\n",
    "config = {\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"vector_store_dir\": \"./vector_store\",\n",
    "    \"llm_provider\": \"openai\",\n",
    "    \"reasoning_llm\": \"gpt-4o\",\n",
    "    \"fast_llm\": \"gpt-4o-mini\",\n",
    "    \"embedding_model\": \"text-embedding-3-small\",\n",
    "    \"reranker_model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"max_reasoning_iterations\": 7, # Maximum loops for the reasoning agent\n",
    "    \"top_k_retrieval\": 10,       # Number of documents for initial broad recall\n",
    "    \"top_n_rerank\": 3,           # Number of documents to keep after precision reranking\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(config[\"data_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"vector_store_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Environment and configuration set up successfully.\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-3-data-pro",
   "metadata": {},
   "source": [
    "### 1.3. The Dataset: Preparing Our Knowledge Base from Complex Documents\n",
    "\n",
    "Our knowledge base will be the full text of NVIDIA's 2023 10-K filing. Instead of a dummy file, we will programmatically download the actual filing from the SEC's EDGAR database. This document is a dense, 100+ page report detailing their business, financials, and risks. This is a perfect test case because answering sophisticated questions requires connecting information spread across disparate sections like 'Business Overview', 'Risk Factors', and 'Management's Discussion and Analysis' (MD&A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part1-3-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing NVIDIA's 2023 10-K filing...\n",
      "Successfully downloaded 10-K filing from https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\n",
      "Raw document saved to ./data/nvda_10k_2023_raw.html\n",
      "Cleaned text content extracted and saved to ./data/nvda_10k_2023_clean.txt\n",
      "--- Sample content from cleaned 10-K ---\n",
      "Item 1. Business. \n",
      " OVERVIEW \n",
      " NVIDIA is the pioneer of accelerated computing. We are a full-stack computing company with a platform strategy that brings together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets we serve. Our work in accelerated computing and AI is reshaping the world’s largest industries and profoundly impacting society. \n",
      " Founded in 1993, we started as a PC graphics chip company, inventing the graphics processing unit, or GPU. The GPU was essential for the growth of the PC gaming market and has since been repurposed to revolutionize computer graphics, high performance computing, or HPC, and AI. \n",
      " The programmability of our GPUs made them ideal for accelerating workloads beyond graphics. In 2006, we invented CUDA, a parallel computing platform and programming model that allows developers to harness the parallel computing power of GPUs for general purpose computing. The combination of our GPUs and CUDA has given rise to the new field of accelerated computing and fueled the AI revolution. \n",
      " Today, our platforms address four large markets where our expertise is critical: Gaming, Professional Visualization, Data Center, and Automotive. Our platforms are built on a common architecture and leverage our core technologies. Our two reportable segments are Compute & Networking and Graphics. Our market platforms are built on a unified architecture but have different engines tailored for their specific use cases. \n",
      "..."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def download_and_parse_10k(url, doc_path_raw, doc_path_clean):\n",
    "    if os.path.exists(doc_path_clean):\n",
    "        print(f\"Cleaned 10-K file already exists at: {doc_path_clean}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading 10-K filing from {url}...\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status() # Ensure we got a valid response\n",
    "    \n",
    "    with open(doc_path_raw, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(f\"Raw document saved to {doc_path_raw}\")\n",
    "    \n",
    "    # Use BeautifulSoup to parse and clean the HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Remove tables, which are often noisy for text-based RAG\n",
    "    for table in soup.find_all('table'):\n",
    "        table.decompose()\n",
    "\n",
    "    # Get clean text, attempting to preserve paragraph breaks\n",
    "    text = ''\n",
    "    for p in soup.find_all(['p', 'div', 'span']):\n",
    "        # Simple heuristic to add newlines between blocks\n",
    "        text += p.get_text(strip=True) + '\\n\\n'\n",
    "    \n",
    "    # A more robust regex to clean up excessive newlines and whitespace\n",
    "    clean_text = re.sub(r'\\n{3,}', '\\n\\n', text).strip()\n",
    "    clean_text = re.sub(r'\\s{2,}', ' ', clean_text).strip()\n",
    "    \n",
    "    with open(doc_path_clean, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "    print(f\"Cleaned text content extracted and saved to {doc_path_clean}\")\n",
    "\n",
    "# URL for NVIDIA's 2023 10-K filing (filed Feb 2023 for fiscal year ending Jan 2023)\n",
    "url_10k = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\"\n",
    "doc_path_raw = os.path.join(config[\"data_dir\"], \"nvda_10k_2023_raw.html\")\n",
    "doc_path_clean = os.path.join(config[\"data_dir\"], \"nvda_10k_2023_clean.txt\")\n",
    "\n",
    "print(\"Downloading and parsing NVIDIA's 2023 10-K filing...\")\n",
    "download_and_parse_10k(url_10k, doc_path_raw, doc_path_clean)\n",
    "\n",
    "with open(doc_path_clean, 'r', encoding='utf-8') as f:\n",
    "    print(\"--- Sample content from cleaned 10-K ---\")\n",
    "    print(f.read(1000) + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-4-challenge-pro-adv",
   "metadata": {},
   "source": [
    "### 1.4. The Upgraded Challenge: A Multi-Source, Multi-Hop Query We Will Conquer\n",
    "\n",
    "This is the query designed to break our baseline RAG system and showcase the power of our advanced agent. It requires the agent to perform multiple distinct information retrieval steps from *different sources* (the static 10-K and the live web) and then synthesize the findings into a coherent analytical narrative.\n",
    "\n",
    "> **The Query:** \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-intro-rag-pro",
   "metadata": {},
   "source": [
    "## Part 2: The Baseline - Building and Breaking a \"Vanilla\" RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-1-dep-pro",
   "metadata": {},
   "source": [
    "### 2.1. Code Dependency: Document Loading and Naive Chunking Strategy\n",
    "\n",
    "Our baseline pipeline begins with a standard approach: load the entire document and split it into fixed-size chunks using a `RecursiveCharacterTextSplitter`. This method is fast but semantically naive, often splitting paragraphs or related ideas across different chunks—a primary source of failure for complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-1-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and chunking the document...\n",
      "Document loaded and split into 378 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Loading and chunking the document...\")\n",
    "loader = TextLoader(doc_path_clean, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Document loaded and split into {len(doc_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-2-dep-pro",
   "metadata": {},
   "source": [
    "### 2.2. Code Dependency: Creating the Vector Store with Dense Embeddings\n",
    "\n",
    "Next, we embed these chunks using OpenAI's `text-embedding-3-small` model and index them in a ChromaDB vector store. This store will power our baseline retriever, which performs a simple semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-2-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating baseline vector store...\n",
      "Vector store created with 378 embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print(\"Creating baseline vector store...\")\n",
    "embedding_function = OpenAIEmbeddings(model=config['embedding_model'])\n",
    "\n",
    "baseline_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "baseline_retriever = baseline_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vector store created with {baseline_vector_store._collection.count()} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-3-dep-pro",
   "metadata": {},
   "source": [
    "### 2.3. Code Dependency: Assembling the Simple RAG Chain\n",
    "\n",
    "We use the LangChain Expression Language (LCEL) to construct our linear pipeline. The `RunnablePassthrough` allows us to pass the original question alongside the retrieved context into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-3-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RAG chain assembled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are an AI financial analyst. Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=config[\"fast_llm\"], temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "baseline_rag_chain = (\n",
    "    {\"context\": baseline_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"Baseline RAG chain assembled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-4-fail-pro-adv",
   "metadata": {},
   "source": [
    "### 2.4. The Critical Failure Case: Demonstrating the Need for Advanced Techniques\n",
    "\n",
    "Now we execute our multi-source query against the baseline system. The retriever will attempt to find chunks that match the 'average' semantic meaning of the entire query. This will fail spectacularly because critical information (about AMD's 2024 strategy) does not exist in its knowledge base (the 2023 10-K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-4-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing complex query on the baseline RAG chain...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BASELINE RAG FAILED OUTPUT ---\n",
      "Based on the provided context, NVIDIA operates in an intensely competitive semiconductor industry and faces competition from companies like AMD. The context mentions that the industry is characterized by rapid technological change. However, the provided documents do not contain any specific information about AMD's recent AI chip strategy from 2024 or how it might impact NVIDIA's stated risks.\n"
     ]
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "console = Console()\n",
    "\n",
    "complex_query_adv = \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\"\n",
    "\n",
    "print(\"Executing complex query on the baseline RAG chain...\")\n",
    "baseline_result = baseline_rag_chain.invoke(complex_query_adv)\n",
    "\n",
    "console.print(\"--- BASELINE RAG FAILED OUTPUT ---\")\n",
    "console.print(Markdown(baseline_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-5-diag-pro-adv",
   "metadata": {},
   "source": [
    "### 2.5. Diagnosis: Why Did It Fail?\n",
    "\n",
    "The output is a classic failure case for RAG systems confined to a static knowledge base.\n",
    "\n",
    "1.  **Irrelevant Context:** The retriever, trying to satisfy all parts of the query at once, likely pulled chunks related to \"competition\" and \"AMD\" from the 10-K, but this information is general and lacks the specifics required.\n",
    "2.  **Missing Information:** The 2023 filing **cannot** contain information about events in 2024. The baseline system has no mechanism to access external, up-to-date knowledge.\n",
    "3.  **No Synthesis:** The system correctly states that it lacks the required information. It cannot perform the requested synthesis because it failed to retrieve one of the two necessary pieces of evidence. It lacks any mechanism to recognize this gap and use a different tool (like web search) to fill it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-intro-deep-pro",
   "metadata": {},
   "source": [
    "## Part 3: The \"Deep Thinking\" Upgrade: Engineering an Autonomous Reasoning Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-1-state-pro-adv",
   "metadata": {},
   "source": [
    "### 3.1. Code Dependency: Defining the `RAGState` - The Central Nervous System of Our Agent\n",
    "\n",
    "To build our reasoning agent, we first need a robust way to manage its state. The `RAGState` `TypedDict` will serve as the central nervous system for our agent. It will be passed between every node in our LangGraph workflow, allowing the agent to maintain a coherent line of reasoning, track its progress, and build a comprehensive base of evidence over multiple steps. We will now enhance our `Step` Pydantic model to include a `tool` field, which will be crucial for routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGState and supporting Pydantic classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Pydantic model for a single step in the reasoning plan\n",
    "class Step(BaseModel):\n",
    "    sub_question: str = Field(description=\"A specific, answerable question for this step.\")\n",
    "    justification: str = Field(description=\"A brief explanation of why this step is necessary to answer the main query.\")\n",
    "    tool: Literal[\"search_10k\", \"search_web\"] = Field(description=\"The tool to use for this step.\")\n",
    "    keywords: List[str] = Field(description=\"A list of critical keywords for searching relevant document sections.\")\n",
    "    document_section: Optional[str] = Field(description=\"A likely document section title (e.g., 'Item 1A. Risk Factors') to search within. Only for 'search_10k' tool.\")\n",
    "\n",
    "# Pydantic model for the overall plan\n",
    "class Plan(BaseModel):\n",
    "    steps: List[Step] = Field(description=\"A detailed, multi-step plan to answer the user's query.\")\n",
    "\n",
    "# TypedDict for storing the results of a completed step\n",
    "class PastStep(TypedDict):\n",
    "    step_index: int\n",
    "    sub_question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    summary: str\n",
    "\n",
    "# The main state dictionary that will flow through the graph\n",
    "class RAGState(TypedDict):\n",
    "    original_question: str\n",
    "    plan: Plan\n",
    "    past_steps: List[PastStep]\n",
    "    current_step_index: int\n",
    "    retrieved_docs: List[Document]\n",
    "    reranked_docs: List[Document]\n",
    "    synthesized_context: str\n",
    "    final_answer: str\n",
    "\n",
    "print(\"RAGState and supporting Pydantic classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-2-planner-pro-adv",
   "metadata": {},
   "source": [
    "### 3.2. Component 1: Dynamic Planning and Query Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-2-1-planner-pro-adv",
   "metadata": {},
   "source": [
    "#### 3.2.1. The Tool-Aware Planner Agent: Decomposing the user query and selecting the right tool for each step.\n",
    "\n",
    "The first cognitive act of our agent is to **plan**. We upgrade our 'Planner Agent' to be **tool-aware**. Its sole responsibility is to take the complex user query and decompose it into a structured, multi-step `Plan` object. Crucially, for each step, it must now decide whether the information is likely to be in the static document (`search_10k`) or requires up-to-date, external information (`search_web`). This decision-making at the planning stage is fundamental to the agent's intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-2-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool-Aware Planner Agent created successfully.\n",
      "--- Testing Planner Agent ---\n",
      "plan:\n",
      "  steps:\n",
      "  - sub_question: What are the key risks related to competition as stated in NVIDIA's\n",
      "      2023 10-K filing?\n",
      "    justification: This step is necessary to extract the foundational information\n",
      "      about competitive risks directly from the source document as requested\n",
      "      by the user.\n",
      "    tool: search_10k\n",
      "    keywords: ['competition', 'risk factors', 'semiconductor industry', 'competitors']\n",
      "    document_section: Item 1A. Risk Factors\n",
      "  - sub_question: What are the recent news and developments in AMD's AI chip strategy\n",
      "      in 2024?\n",
      "    justification: This step requires finding up-to-date, external information\n",
      "      that is not available in the 2023 10-K filing. A web search is necessary\n",
      "      to get the latest details on AMD's strategy.\n",
      "    tool: search_web\n",
      "    keywords: ['AMD', 'AI chip strategy', '2024', 'MI300X', 'Instinct accelerator']\n",
      "    document_section: null\n",
      "  - sub_question: How does AMD's 2024 AI chip strategy potentially exacerbate the\n",
      "      competitive risks identified in NVIDIA's 10-K?\n",
      "    justification: This final step synthesizes the information from the internal\n",
      "      document (NVIDIA's risks) and the external web search (AMD's strategy)\n",
      "      to provide a comprehensive answer to the user's analytical question.\n",
      "    tool: search_10k\n",
      "    keywords: ['impact', 'threaten', 'competitive pressure', 'market share',\n",
      "      'technological change']\n",
      "    document_section: Item 1A. Risk Factors\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from rich.pretty import pprint as rprint\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert research planner. Your task is to create a clear, multi-step plan to answer a complex user query by retrieving information from multiple sources.\n",
    "You have two tools available:\n",
    "1. `search_10k`: Use this to search for information within NVIDIA's 2023 10-K financial filing. This is best for historical facts, financial data, and stated company policies or risks from that specific time period.\n",
    "2. `search_web`: Use this to search the public internet for recent news, competitor information, or any topic that is not specific to NVIDIA's 2023 10-K.\n",
    "\n",
    "Decompose the user's query into a series of simple, sequential sub-questions. For each step, decide which tool is more appropriate.\n",
    "For `search_10k` steps, also identify the most likely section of the 10-K (e.g., 'Item 1A. Risk Factors', 'Item 7. Management’s Discussion and Analysis...').\n",
    "It is critical to use the exact section titles found in a 10-K filing where possible.\"\"\"),\n",
    "    (\"human\", \"User Query: {question}\")\n",
    "])\n",
    "\n",
    "reasoning_llm = ChatOpenAI(model=config[\"reasoning_llm\"], temperature=0)\n",
    "planner_agent = planner_prompt | reasoning_llm.with_structured_output(Plan)\n",
    "print(\"Tool-Aware Planner Agent created successfully.\")\n",
    "\n",
    "# Test the planner agent\n",
    "print(\"--- Testing Planner Agent ---\")\n",
    "test_plan = planner_agent.invoke({\"question\": complex_query_adv})\n",
    "rprint(test_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-2-2-rewriter-pro",
   "metadata": {},
   "source": [
    "#### 3.2.2. Query Rewriting and Expansion: Using an LLM to transform naive sub-questions into high-quality search queries.\n",
    "\n",
    "A sub-question from the plan (e.g., \"What are the risks?\") might not be the optimal query for a vector database or web search engine. We create a 'Query Rewriter' agent that enriches the sub-question with keywords from the plan and context from previous steps, making it a much more effective search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-2-2-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Rewriter Agent created successfully.\n",
      "--- Testing Query Rewriter Agent ---\n",
      "Original sub-question: How does AMD's 2024 AI chip strategy potentially exacerbate the competitive risks identified in NVIDIA's 10-K?\n",
      "Rewritten Search Query: analysis of how AMD's 2024 AI chip strategy, including products like the MI300X, exacerbates NVIDIA's stated competitive risks such as rapid technological change and market share erosion in the data center and AI semiconductor industry\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "query_rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a search query optimization expert. Your task is to rewrite a given sub-question into a highly effective search query for a vector database or web search engine, using keywords and context from the research plan.\n",
    "The rewritten query should be specific, use terminology likely to be found in the target source (a financial 10-K or news articles), and be structured to retrieve the most relevant text snippets.\"\"\"),\n",
    "    (\"human\", \"Current sub-question: {sub_question}\\n\\nRelevant keywords from plan: {keywords}\\n\\nContext from past steps:\\n{past_context}\")\n",
    "])\n",
    "\n",
    "query_rewriter_agent = query_rewriter_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Query Rewriter Agent created successfully.\")\n",
    "\n",
    "# Test the rewriter agent\n",
    "print(\"--- Testing Query Rewriter Agent ---\")\n",
    "test_sub_q = test_plan.steps[2] # The synthesis step\n",
    "test_past_context = \"Step 1 Summary: NVIDIA's 10-K lists intense competition and rapid technological change as key risks. Step 2 Summary: AMD launched its MI300X AI accelerator in 2024 to directly compete with NVIDIA's H100.\"\n",
    "rewritten_q = query_rewriter_agent.invoke({\n",
    "    \"sub_question\": test_sub_q.sub_question,\n",
    "    \"keywords\": test_sub_q.keywords,\n",
    "    \"past_context\": test_past_context\n",
    "})\n",
    "print(f\"Original sub-question: {test_sub_q.sub_question}\")\n",
    "print(f\"Rewritten Search Query: {rewritten_q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-2-3-metadata-pro",
   "metadata": {},
   "source": [
    "#### 3.2.3. Entity and Constraint Extraction: Identifying metadata filters to enable filtered vector search.\n",
    "\n",
    "This is a crucial step for precision when using the `search_10k` tool. Our planner already extracts the likely `document_section`. To use this, we need to re-process our documents, adding this section title as metadata to each chunk. This allows us to perform a *filtered search*, telling the vector store to *only* search within chunks that have the correct metadata (e.g., only search for risks in the 'Risk Factors' section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-2-3-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and adding metadata...\n",
      "Identified 22 document sections.\n",
      "Created 381 chunks with section metadata.\n",
      "--- Sample Chunk with Metadata ---\n",
      "page_content: 'Our industry is intensely competitive. We operate in the semiconductor\\nindustry, which is intensely competitive and characterized by rapid\\ntechnological change and evolving industry standards. We compete with a number of\\ncompanies that have different business models and different combinations of\\nhardware, software, and systems expertise, many of which have substantially\\ngreater resources than we have. We expect competition to increase from existing\\ncompetitors, as well as new and emerging companies. Our competitors include\\nIntel, AMD, and Qualcomm; cloud service providers, or CSPs, such as Amazon Web\\nServices, or AWS, Google Cloud, and Microsoft Azure; and various companies\\ndeveloping or that may develop processors or systems for the AI, HPC, data\\ncenter, gaming, professional visualization, and automotive markets. Some of our\\ncustomers are also our competitors. Our business could be materially and\\nadversely affected if our competitors announce or introduce new products, services,\\nor technologies that have better performance or features, are less expensive, or\\nthat gain market acceptance.'\n",
      "metadata: {'section': 'Item 1A. Risk Factors.', 'source_doc':\\n           './data/nvda_10k_2023_clean.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing document and adding metadata...\")\n",
    "# Regex to match the 'Item X' and 'Item X.Y' patterns for section titles\n",
    "section_pattern = r\"(ITEM\\s+\\d[A-Z]?\\.\\s*.*?)(?=\\nITEM\\s+\\d[A-Z]?\\.|$)\"\n",
    "raw_text = documents[0].page_content\n",
    "\n",
    "# Find all matches for section titles\n",
    "section_titles = re.findall(section_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "section_titles = [title.strip().replace('\\n', ' ') for title in section_titles]\n",
    "\n",
    "# Split the document content by these titles\n",
    "sections_content = re.split(section_pattern, raw_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "sections_content = [content.strip() for content in sections_content if content.strip() and not content.strip().lower().startswith('item ')]\n",
    "\n",
    "print(f\"Identified {len(section_titles)} document sections.\")\n",
    "assert len(section_titles) == len(sections_content), \"Mismatch between titles and content sections\"\n",
    "\n",
    "doc_chunks_with_metadata = []\n",
    "for i, content in enumerate(sections_content):\n",
    "    section_title = section_titles[i]\n",
    "    # Chunk the content of this specific section\n",
    "    section_chunks = text_splitter.split_text(content)\n",
    "    for chunk in section_chunks:\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        doc_chunks_with_metadata.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"section\": section_title,\n",
    "                    \"source_doc\": doc_path_clean,\n",
    "                    \"id\": chunk_id\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Created {len(doc_chunks_with_metadata)} chunks with section metadata.\")\n",
    "print(\"--- Sample Chunk with Metadata ---\")\n",
    "sample_chunk = next(c for c in doc_chunks_with_metadata if \"Risk Factors\" in c.metadata.get(\"section\", \"\"))\n",
    "rprint(sample_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-retrieval-pro-adv",
   "metadata": {},
   "source": [
    "### 3.3. Component 2: The Multi-Stage, Adaptive Retrieval Funnel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-1-supervisor-pro",
   "metadata": {},
   "source": [
    "#### 3.3.1. NEW: The Retrieval Supervisor Agent\n",
    "\n",
    "This is a new, crucial component for intelligent retrieval. Not all questions are created equal. Some benefit from semantic search (e.g., \"What are the company's feelings on climate change?\"), while others are better with keyword search (e.g., \"What was the revenue for the 'Compute & Networking' segment?\").\n",
    "\n",
    "The **Retrieval Supervisor** is a small LLM agent that acts as a router. For each `search_10k` step, it analyzes the sub-question and decides which retrieval strategy—`vector_search`, `keyword_search`, or `hybrid_search`—is most appropriate. This adds a layer of dynamic decision-making that optimizes the retrieval process for each specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-3-1-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Supervisor Agent created.\n",
      "--- Testing Retrieval Supervisor Agent ---\n",
      "Query: 'revenue growth for the Compute & Networking segment in fiscal year 2023'\n",
      "Decision: keyword_search, Justification: The query contains specific keywords like 'revenue growth', 'Compute & Networking', and 'fiscal year 2023' which are ideal for a keyword-based search to find exact financial figures.\n",
      "Query: 'general sentiment about market competition and technological innovation'\n",
      "Decision: vector_search, Justification: This query is conceptual and seeks to understand sentiment and broader themes. Vector search is better suited to capture the semantic meaning of 'market competition' and 'technological innovation' rather than relying on exact keywords.\n"
     ]
    }
   ],
   "source": [
    "class RetrievalDecision(BaseModel):\n",
    "    strategy: Literal[\"vector_search\", \"keyword_search\", \"hybrid_search\"]\n",
    "    justification: str\n",
    "\n",
    "retrieval_supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a retrieval strategy expert. Based on the user's query, you must decide the best retrieval strategy.\n",
    "You have three options:\n",
    "1. `vector_search`: Best for conceptual, semantic, or similarity-based queries.\n",
    "2. `keyword_search`: Best for queries with specific, exact terms, names, or codes (e.g., 'Item 1A', 'Hopper architecture').\n",
    "3. `hybrid_search`: A good default that combines both, but may be less precise than a targeted strategy.\"\"\"),\n",
    "    (\"human\", \"User Query: {sub_question}\")\n",
    "])\n",
    "\n",
    "retrieval_supervisor_agent = retrieval_supervisor_prompt | reasoning_llm.with_structured_output(RetrievalDecision)\n",
    "print(\"Retrieval Supervisor Agent created.\")\n",
    "\n",
    "# Test the supervisor\n",
    "print(\"--- Testing Retrieval Supervisor Agent ---\")\n",
    "query1 = \"revenue growth for the Compute & Networking segment in fiscal year 2023\"\n",
    "decision1 = retrieval_supervisor_agent.invoke({\"sub_question\": query1})\n",
    "print(f\"Query: '{query1}'\")\n",
    "print(f\"Decision: {decision1.strategy}, Justification: {decision1.justification}\")\n",
    "\n",
    "query2 = \"general sentiment about market competition and technological innovation\"\n",
    "decision2 = retrieval_supervisor_agent.invoke({\"sub_question\": query2})\n",
    "print(f\"Query: '{query2}'\")\n",
    "print(f\"Decision: {decision2.strategy}, Justification: {decision2.justification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-2-strategies-pro",
   "metadata": {},
   "source": [
    "#### 3.3.2. Implementing the Retrieval Strategies\n",
    "\n",
    "Now we build our advanced retriever. We create a new vector store with our metadata-rich chunks. We then implement three distinct search functions: pure vector search, pure keyword search (BM25), and a hybrid approach that fuses the results using Reciprocal Rank Fusion (RRF). Our `retrieval_node` in the graph will use the decision from the `RetrievalSupervisor` to call the appropriate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-3-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced vector store with metadata...\n",
      "Advanced vector store created with 381 embeddings.\n",
      "Building BM25 index for keyword search...\n",
      "All retrieval strategy functions ready.\n",
      "--- Testing Keyword Search ---\n",
      "Query: Item 1A. Risk Factors\n",
      "Found 10 documents. Top result section: Item 1A. Risk Factors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "print(\"Creating advanced vector store with metadata...\")\n",
    "advanced_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks_with_metadata,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "print(f\"Advanced vector store created with {advanced_vector_store._collection.count()} embeddings.\")\n",
    "\n",
    "print(\"Building BM25 index for keyword search...\")\n",
    "tokenized_corpus = [doc.page_content.split(\" \") for doc in doc_chunks_with_metadata]\n",
    "doc_ids = [doc.metadata[\"id\"] for doc in doc_chunks_with_metadata]\n",
    "doc_map = {doc.metadata[\"id\"]: doc for doc in doc_chunks_with_metadata}\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def vector_search_only(query: str, section_filter: str = None, k: int = 10):\n",
    "    filter_dict = {\"section\": section_filter} if section_filter and \"Unknown\" not in section_filter else None\n",
    "    return advanced_vector_store.similarity_search(query, k=k, filter=filter_dict)\n",
    "\n",
    "def bm25_search_only(query: str, k: int = 10):\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    return [doc_map[doc_ids[i]] for i in top_k_indices]\n",
    "\n",
    "def hybrid_search(query: str, section_filter: str = None, k: int = 10):\n",
    "    # 1. Keyword Search (BM25)\n",
    "    bm25_docs = bm25_search_only(query, k=k)\n",
    "\n",
    "    # 2. Semantic Search (with metadata filtering)\n",
    "    semantic_docs = vector_search_only(query, section_filter=section_filter, k=k)\n",
    "\n",
    "    # 3. Reciprocal Rank Fusion (RRF)\n",
    "    all_docs = {doc.metadata[\"id\"]: doc for doc in bm25_docs + semantic_docs}.values()\n",
    "    ranked_lists = [[doc.metadata[\"id\"] for doc in bm25_docs], [doc.metadata[\"id\"] for doc in semantic_docs]]\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    for doc_list in ranked_lists:\n",
    "        for i, doc_id in enumerate(doc_list):\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = 0\n",
    "            rrf_scores[doc_id] += 1 / (i + 61) # RRF rank constant k = 60\n",
    "\n",
    "    sorted_doc_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "    final_docs = [doc_map[doc_id] for doc_id in sorted_doc_ids[:k]]\n",
    "    return final_docs\n",
    "\n",
    "print(\"All retrieval strategy functions ready.\")\n",
    "\n",
    "# Test Keyword Search\n",
    "print(\"--- Testing Keyword Search ---\")\n",
    "test_query = \"Item 1A. Risk Factors\"\n",
    "test_results = bm25_search_only(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Found {len(test_results)} documents. Top result section: {test_results[0].metadata['section']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-3-reranker-pro",
   "metadata": {},
   "source": [
    "#### 3.3.3. Stage 2 (High Precision): Cross-Encoder Reranker.\n",
    "\n",
    "After retrieving a broad set of `k` documents, we use a more computationally expensive but far more accurate **Cross-Encoder** model. Unlike embedding models (bi-encoders) that create vectors independently, a cross-encoder processes the query and each document *together*, yielding a much more nuanced relevance score. This allows us to re-rank the `k` candidates and select the top `n` with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-3-2-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CrossEncoder reranker...\n",
      "Cross-Encoder ready.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "print(\"Initializing CrossEncoder reranker...\")\n",
    "reranker = CrossEncoder(config[\"reranker_model\"])\n",
    "\n",
    "def rerank_documents_function(query: str, documents: List[Document]) -> List[Document]:\n",
    "    if not documents: return []\n",
    "    pairs = [(query, doc.page_content) for doc in documents]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Combine documents with their scores and sort\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top N documents\n",
    "    reranked_docs = [doc for doc, score in doc_scores[:config[\"top_n_rerank\"]]]\n",
    "    return reranked_docs\n",
    "\n",
    "print(\"Cross-Encoder ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-3-4-distill-pro",
   "metadata": {},
   "source": [
    "#### 3.3.4. Stage 3 (Contextual Distillation): Implementing logic to synthesize a concise context.\n",
    "\n",
    "The final step in our retrieval funnel is to distill the top `n` highly relevant chunks into a single, clean paragraph of context. This removes redundancy and presents the information to the downstream agents in a clean, easy-to-process format. We create a dedicated 'Distiller Agent' for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-3-3-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Distiller Agent created.\n"
     ]
    }
   ],
   "source": [
    "distiller_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant. Your task is to synthesize the following retrieved document snippets into a single, concise paragraph.\n",
    "The goal is to provide a clear and coherent context that directly answers the question: '{question}'.\n",
    "Focus on removing redundant information and organizing the content logically. Answer only with the synthesized context.\"\"\"),\n",
    "    (\"human\", \"Retrieved Documents:\\n{context}\")\n",
    "])\n",
    "\n",
    "distiller_agent = distiller_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Contextual Distiller Agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-4-tool-pro",
   "metadata": {},
   "source": [
    "### 3.4. Component 3: Tool Augmentation with Web Search\n",
    "\n",
    "To answer questions about recent events or competitors, our agent needs to break out of its static knowledge base. We equip it with a web search tool using the Tavily Search API. The `planner_agent` will decide when to invoke this tool. The results from the web search will be formatted into LangChain `Document` objects, allowing them to be processed by the same reranking and compression pipeline as the documents retrieved from our vector store. This ensures a seamless integration of internal and external knowledge sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-4-code-pro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search tool (Tavily) initialized.\n",
      "--- Testing Web Search Tool ---\n",
      "Found 3 results for query: 'AMD AI chip strategy 2024'\n",
      "Top result snippet: AMD has intensified its battle with Nvidia in the AI chip market with the release of the Instinct MI300X accelerator, a powerful GPU designed to challenge Nvidia's H100 in training and inference for large language models. Major cloud providers like Microsoft Azure and Oracle Cloud are adopting the MI300X, indicating strong market interest...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search_function(query: str) -> List[Document]:\n",
    "    results = web_search_tool.invoke({\"query\": query})\n",
    "    return [Document(page_content=res[\"content\"], metadata={\"source\": res[\"url\"]}) for res in results]\n",
    "\n",
    "print(\"Web search tool (Tavily) initialized.\")\n",
    "\n",
    "# Test the web search\n",
    "print(\"--- Testing Web Search Tool ---\")\n",
    "test_query_web = \"AMD AI chip strategy 2024\"\n",
    "test_results_web = web_search_function(test_query_web)\n",
    "print(f\"Found {len(test_results_web)} results for query: '{test_query_web}'\")\n",
    "if test_results_web:\n",
    "    print(f\"Top result snippet: {test_results_web[0].page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-5-critique-pro",
   "metadata": {},
   "source": [
    "### 3.5. Component 4: The Self-Critique and Control Flow Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-5-1-reflect-pro",
   "metadata": {},
   "source": [
    "#### 3.5.1. The \"Update and Reflect\" Step: An agent that synthesizes new findings into the `RAGState`'s reasoning history.\n",
    "\n",
    "After each retrieval loop, the agent needs to integrate its new knowledge. The 'Reflection Agent' takes the distilled context from the current step and creates a concise summary. This summary is then appended to the `past_steps` list in our `RAGState`, forming a cumulative log of the agent's research journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-4-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflection Agent created.\n"
     ]
    }
   ],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a research assistant. Based on the retrieved context for the current sub-question, write a concise, one-sentence summary of the key findings.\n",
    "This summary will be added to our research history. Be factual and to the point.\"\"\"),\n",
    "    (\"human\", \"Current sub-question: {sub_question}\\n\\nDistilled context:\\n{context}\")\n",
    "])\n",
    "reflection_agent = reflection_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Reflection Agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-5-2-policy-pro",
   "metadata": {},
   "source": [
    "#### 3.5.2. Policy Implementation (LLM-as-a-Judge): Prompting an LLM to inspect the current state and decide the next action.\n",
    "\n",
    "This is the cognitive core of our agent's autonomy. The 'Policy Agent' acts as a supervisor. After each reflection step, it examines the *entire* research history (`past_steps`) in relation to the original question and the plan. It then makes a structured decision: `CONTINUE_PLAN` if more information is needed, or `FINISH` if the question has been comprehensively answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-4-2-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent created.\n",
      "--- Testing Policy Agent (Incomplete State) ---\n",
      "Decision: CONTINUE_PLAN, Justification: The research has only identified NVIDIA's competitive risks from the 10-K. It has not yet gathered the required external information about AMD's 2024 strategy, which is the next step in the plan.\n",
      "--- Testing Policy Agent (Complete State) ---\n",
      "Decision: FINISH, Justification: The research history now contains comprehensive summaries of both NVIDIA's stated competitive risks and AMD's recent AI chip strategy. All necessary information has been gathered to perform the final synthesis and answer the user's question.\n"
     ]
    }
   ],
   "source": [
    "class Decision(BaseModel):\n",
    "    next_action: Literal[\"CONTINUE_PLAN\", \"FINISH\"]\n",
    "    justification: str\n",
    "\n",
    "policy_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a master strategist. Your role is to analyze the research progress and decide the next action.\n",
    "You have the original question, the initial plan, and a log of completed steps with their summaries.\n",
    "- If the collected information in the Research History is sufficient to comprehensively answer the Original Question, decide to FINISH.\n",
    "- Otherwise, if the plan is not yet complete, decide to CONTINUE_PLAN.\"\"\"),\n",
    "    (\"human\", \"Original Question: {question}\\n\\nInitial Plan:\\n{plan}\\n\\nResearch History (Completed Steps):\\n{history}\")\n",
    "])\n",
    "policy_agent = policy_prompt | reasoning_llm.with_structured_output(Decision)\n",
    "print(\"Policy Agent created.\")\n",
    "\n",
    "# Test the policy agent with different states\n",
    "plan_str = json.dumps([s.dict() for s in test_plan.steps])\n",
    "incomplete_history = \"Step 1 Summary: NVIDIA's 10-K states that the semiconductor industry is intensely competitive and subject to rapid technological change.\"\n",
    "decision1 = policy_agent.invoke({\"question\": complex_query_adv, \"plan\": plan_str, \"history\": incomplete_history})\n",
    "print(\"--- Testing Policy Agent (Incomplete State) ---\")\n",
    "print(f\"Decision: {decision1.next_action}, Justification: {decision1.justification}\")\n",
    "\n",
    "complete_history = incomplete_history + \"\\nStep 2 Summary: In 2024, AMD launched its MI300X accelerator to directly compete with NVIDIA in the AI chip market, gaining adoption from major cloud providers.\"\n",
    "decision2 = policy_agent.invoke({\"question\": complex_query_adv, \"plan\": plan_str, \"history\": complete_history})\n",
    "print(\"--- Testing Policy Agent (Complete State) ---\")\n",
    "print(f\"Decision: {decision2.next_action}, Justification: {decision2.justification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-5-3-stopping-pro",
   "metadata": {},
   "source": [
    "#### 3.5.3. Defining Robust Stopping Criteria\n",
    "\n",
    "Our system needs clear and robust conditions to stop the reasoning loop. We have three such criteria:\n",
    "1.  **Policy Decision:** The primary stopping condition is when the `policy_agent` confidently decides to `FINISH`.\n",
    "2.  **Plan Completion:** If the agent has executed every step in its plan, it will naturally conclude its work.\n",
    "3.  **Max Iterations:** As a safeguard against infinite loops or runaway processes, we enforce a hard limit (`max_reasoning_iterations` from our config) on the number of research cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-intro-graph-pro",
   "metadata": {},
   "source": [
    "## Part 4: Assembly with LangGraph - Orchestrating the Reasoning Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-1-nodes-pro-adv",
   "metadata": {},
   "source": [
    "### 4.1. Code Dependency: Defining the Graph Nodes\n",
    "\n",
    "Now, we translate our conceptual components into concrete graph nodes. Each node is a Python function that accepts the `RAGState` dictionary, performs its designated task, and returns a dictionary containing the state updates. We add a new `web_search_node` to handle the external search tool, and we modify the `retrieval_node` to incorporate the adaptive strategy chosen by our new Supervisor agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All graph nodes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def get_past_context_str(past_steps: List[PastStep]) -> str:\n",
    "    return \"\\n\\n\".join([f\"Step {s['step_index']}: {s['sub_question']}\\nSummary: {s['summary']}\" for s in past_steps])\n",
    "\n",
    "def plan_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- 🧠: Generating Plan ---\")\n",
    "    plan = planner_agent.invoke({\"question\": state[\"original_question\"]})\n",
    "    rprint(plan)\n",
    "    return {\"plan\": plan, \"current_step_index\": 0, \"past_steps\": []}\n",
    "\n",
    "def retrieval_node(state: RAGState) -> Dict:\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    console.print(f\"--- 🔍: Retrieving from 10-K (Step {current_step_index + 1}: {current_step.sub_question}) ---\")\n",
    "    past_context = get_past_context_str(state['past_steps'])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    console.print(f\"  Rewritten Query: {rewritten_query}\")\n",
    "    \n",
    "    # NEW: Adaptive Retrieval Strategy\n",
    "    retrieval_decision = retrieval_supervisor_agent.invoke({\"sub_question\": rewritten_query})\n",
    "    console.print(f\"  Supervisor Decision: Use `{retrieval_decision.strategy}`. Justification: {retrieval_decision.justification}\")\n",
    "\n",
    "    if retrieval_decision.strategy == 'vector_search':\n",
    "        retrieved_docs = vector_search_only(rewritten_query, section_filter=current_step.document_section, k=config['top_k_retrieval'])\n",
    "    elif retrieval_decision.strategy == 'keyword_search':\n",
    "        retrieved_docs = bm25_search_only(rewritten_query, k=config['top_k_retrieval'])\n",
    "    else: # hybrid_search\n",
    "        retrieved_docs = hybrid_search(rewritten_query, section_filter=current_step.document_section, k=config['top_k_retrieval'])\n",
    "    \n",
    "    return {\"retrieved_docs\": retrieved_docs}\n",
    "\n",
    "def web_search_node(state: RAGState) -> Dict:\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    console.print(f\"--- 🌐: Searching Web (Step {current_step_index + 1}: {current_step.sub_question}) ---\")\n",
    "    past_context = get_past_context_str(state['past_steps'])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    console.print(f\"  Rewritten Query: {rewritten_query}\")\n",
    "    retrieved_docs = web_search_function(rewritten_query)\n",
    "    return {\"retrieved_docs\": retrieved_docs}\n",
    "\n",
    "def rerank_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- 🎯: Reranking Documents ---\")\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    reranked_docs = rerank_documents_function(current_step.sub_question, state[\"retrieved_docs\"])\n",
    "    console.print(f\"  Reranked to top {len(reranked_docs)} documents.\")\n",
    "    return {\"reranked_docs\": reranked_docs}\n",
    "\n",
    "def compression_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- ✂️: Distilling Context ---\")\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    context = format_docs(state[\"reranked_docs\"])\n",
    "    synthesized_context = distiller_agent.invoke({\"question\": current_step.sub_question, \"context\": context})\n",
    "    console.print(f\"  Distilled Context Snippet: {synthesized_context[:200]}...\")\n",
    "    return {\"synthesized_context\": synthesized_context}\n",
    "\n",
    "def reflection_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- 🤔: Reflecting on Findings ---\")\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    summary = reflection_agent.invoke({\"sub_question\": current_step.sub_question, \"context\": state['synthesized_context']})\n",
    "    console.print(f\"  Summary: {summary}\")\n",
    "    new_past_step = {\n",
    "        \"step_index\": current_step_index + 1,\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"retrieved_docs\": state['reranked_docs'],\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    return {\"past_steps\": state[\"past_steps\"] + [new_past_step], \"current_step_index\": current_step_index + 1}\n",
    "\n",
    "def final_answer_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- ✅: Generating Final Answer with Citations ---\")\n",
    "    # Create a consolidated context with metadata for citation\n",
    "    final_context = \"\"\n",
    "    for i, step in enumerate(state['past_steps']):\n",
    "        final_context += f\"\\n--- Findings from Research Step {i+1} ---\\n\"\n",
    "        for doc in step['retrieved_docs']:\n",
    "            source = doc.metadata.get('section') or doc.metadata.get('source')\n",
    "            final_context += f\"Source: {source}\\nContent: {doc.page_content}\\n\\n\"\n",
    "    \n",
    "    final_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert financial analyst. Synthesize the research findings from internal documents and web searches into a comprehensive, multi-paragraph answer for the user's original question.\n",
    "Your answer must be grounded in the provided context. At the end of any sentence that relies on specific information, you MUST add a citation. For 10-K documents, use [Source: <section title>]. For web results, use [Source: <URL>].\"\"\"),\n",
    "        (\"human\", \"Original Question: {question}\\n\\nResearch History and Context:\\n{context}\")\n",
    "    ])\n",
    "    \n",
    "    final_answer_agent = final_answer_prompt | reasoning_llm | StrOutputParser()\n",
    "    final_answer = final_answer_agent.invoke({\"question\": state['original_question'], \"context\": final_context})\n",
    "    return {\"final_answer\": final_answer}\n",
    "\n",
    "print(\"All graph nodes defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-2-edges-pro-adv",
   "metadata": {},
   "source": [
    "### 4.2. Code Dependency: Defining the Conditional Edges - Implementing the Self-Critique Policy Logic\n",
    "\n",
    "We now define the logic that controls the flow of our graph. We add a `route_by_tool` function that checks the plan and directs the agent to either the `retrieval_node` or the `web_search_node`. The `should_continue_node` remains the primary controller for the main reasoning loop, implementing our stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-2-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional edge logic functions defined.\n"
     ]
    }
   ],
   "source": [
    "def route_by_tool(state: RAGState) -> str:\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    return current_step.tool\n",
    "\n",
    "def should_continue_node(state: RAGState) -> str:\n",
    "    console.print(\"--- 🚦: Evaluating Policy ---\")\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    \n",
    "    if current_step_index >= len(state[\"plan\"].steps):\n",
    "        console.print(\"  -> Plan complete. Finishing.\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    if current_step_index >= config[\"max_reasoning_iterations\"]:\n",
    "        console.print(\"  -> Max iterations reached. Finishing.\")\n",
    "        return \"finish\"\n",
    "\n",
    "    # Check if the last retrieval step failed to find documents\n",
    "    if not state[\"reranked_docs\"]:\n",
    "        console.print(\"  -> Retrieval failed for the last step. Continuing with next step in plan.\")\n",
    "        return \"continue\"\n",
    "\n",
    "    history = get_past_context_str(state['past_steps'])\n",
    "    plan_str = json.dumps([s.dict() for s in state['plan'].steps])\n",
    "    decision = policy_agent.invoke({\"question\": state[\"original_question\"], \"plan\": plan_str, \"history\": history})\n",
    "    console.print(f\"  -> Decision: {decision.next_action} | Justification: {decision.justification}\")\n",
    "    \n",
    "    if decision.next_action == \"FINISH\":\n",
    "        return \"finish\"\n",
    "    else: # CONTINUE_PLAN\n",
    "        return \"continue\"\n",
    "\n",
    "print(\"Conditional edge logic functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-3-build-pro-adv",
   "metadata": {},
   "source": [
    "### 4.3. Building the `StateGraph`: Wiring the Deep Thinking RAG Machine\n",
    "\n",
    "Now we instantiate the `StateGraph` and assemble our more advanced cognitive architecture. The key change is adding a conditional entry point after the `plan` node. This `route_by_tool` edge will direct the agent to the correct tool for the current step. After each tool execution and subsequent processing, the graph flows to the `reflect` node, which then loops back to the tool router for the next step in the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-3-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateGraph constructed successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"plan\", plan_node)\n",
    "graph.add_node(\"retrieve_10k\", retrieval_node)\n",
    "graph.add_node(\"retrieve_web\", web_search_node)\n",
    "graph.add_node(\"rerank\", rerank_node)\n",
    "graph.add_node(\"compress\", compression_node)\n",
    "graph.add_node(\"reflect\", reflection_node)\n",
    "graph.add_node(\"generate_final_answer\", final_answer_node)\n",
    "\n",
    "# Define edges\n",
    "graph.set_entry_point(\"plan\")\n",
    "graph.add_conditional_edges(\n",
    "    \"plan\",\n",
    "    route_by_tool,\n",
    "    {\n",
    "        \"search_10k\": \"retrieve_10k\",\n",
    "        \"search_web\": \"retrieve_web\",\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"retrieve_10k\", \"rerank\")\n",
    "graph.add_edge(\"retrieve_web\", \"rerank\")\n",
    "graph.add_edge(\"rerank\", \"compress\")\n",
    "graph.add_edge(\"compress\", \"reflect\")\n",
    "graph.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    should_continue_node,\n",
    "    {\n",
    "        \"continue\": \"plan\", # Re-evaluate plan for next step's tool\n",
    "        \"finish\": \"generate_final_answer\",\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"generate_final_answer\", END)\n",
    "\n",
    "print(\"StateGraph constructed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-4-viz-pro-adv",
   "metadata": {},
   "source": [
    "### 4.4. Compiling and Visualizing the Iterative Workflow\n",
    "\n",
    "The final step is to compile our graph definition into an executable `Runnable`. We then generate a visual diagram of the graph. The new diagram will clearly show the branching logic where the agent decides between its internal knowledge base (`retrieve_10k`) and its external web search tool (`retrieve_web`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-4-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA... (large image data) ...AAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deep_thinking_rag_graph = graph.compile()\n",
    "print(\"Graph compiled successfully.\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    # Correctly call get_graph() before draw_png()\n",
    "    png_image = deep_thinking_rag_graph.get_graph().draw_png()\n",
    "    display(Image(png_image))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}. Please ensure pygraphviz is installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-intro-redemption-pro",
   "metadata": {},
   "source": [
    "## Part 5: Redemption - Running the Deep Thinking Pipeline on Our Challenge Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-1-invoke-pro-adv",
   "metadata": {},
   "source": [
    "### 5.1. Invoking the Graph: A Step-by-Step Trace of the Full Reasoning Process\n",
    "\n",
    "With our graph compiled, we can now invoke it with our complex, multi-source query. We use the `.stream()` method to observe the agent's execution in real-time. The trace will now demonstrate the agent's ability to first query its internal knowledge base, and then seamlessly switch to its web search tool to gather the external information required to fully answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part5-1-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Invoking Deep Thinking RAG Graph ---\n",
      "--- 🧠: Generating Plan ---\n",
      "plan:\n",
      "  steps:\n",
      "  - sub_question: What are the key risks related to competition as stated in NVIDIA's 2023 10-K filing?\n",
      "    justification: This step is necessary to extract the foundational information about competitive risks directly from the source document as requested by the user.\n",
      "    tool: search_10k\n",
      "    keywords: ['competition', 'risk factors', 'semiconductor industry', 'competitors']\n",
      "    document_section: 'Item 1A. Risk Factors.'\n",
      "  - sub_question: What are the recent news and developments in AMD's AI chip strategy in 2024?\n",
      "    justification: This step requires finding up-to-date, external information that is not available in the 2023 10-K filing. A web search is necessary to get the latest details on AMD's strategy.\n",
      "    tool: search_web\n",
      "    keywords: ['AMD', 'AI chip strategy', '2024', 'MI300X', 'Instinct accelerator']\n",
      "    document_section: null\n",
      "--- 🔍: Retrieving from 10-K (Step 1: What are the key risks related to competition as stated in NVIDIA's 2023 10-K filing?) ---\n",
      "  Rewritten Query: key competitive risks for NVIDIA in the semiconductor industry as detailed in the 'Item 1A. Risk Factors' section of the 2023 10-K filing\n",
      "  Supervisor Decision: Use `hybrid_search`. Justification: The query is a mix of a specific section title ('Item 1A. Risk Factors') and conceptual terms ('competitive risks'), making hybrid search the optimal strategy to balance precision and semantic relevance.\n",
      "--- 🎯: Reranking Documents ---\n",
      "  Reranked to top 3 documents.\n",
      "--- ✂️: Distilling Context ---\n",
      "  Distilled Context Snippet: NVIDIA operates in the intensely competitive semiconductor industry, which is characterized by rapid technological change and evolving standards. The company faces competition from numerous companies, some wit...\n",
      "--- 🤔: Reflecting on Findings ---\n",
      "  Summary: According to its 2023 10-K, NVIDIA operates in an intensely competitive semiconductor industry marked by rapid technological change, facing rivals with substantial resources who could introduce superior or cheaper products.\n",
      "--- 🚦: Evaluating Policy ---\n",
      "  -> Decision: CONTINUE_PLAN | Justification: The first step of identifying NVIDIA's competitive risks has been completed. The next step, which is to find recent news about AMD's AI chip strategy, is still pending and necessary to answer the full query.\n",
      "--- 🌐: Searching Web (Step 2: What are the recent news and developments in AMD's AI chip strategy in 2024?) ---\n",
      "  Rewritten Query: AMD AI chip strategy news and developments 2024 Instinct MI300X accelerator competition with NVIDIA\n",
      "--- 🎯: Reranking Documents ---\n",
      "  Reranked to top 3 documents.\n",
      "--- ✂️: Distilling Context ---\n",
      "  Distilled Context Snippet: AMD has ramped up its challenge to Nvidia in the AI accelerator market with its Instinct MI300 series, particularly the MI300X GPU. Launched in late 2023 and shipping in volume in 2024, the MI300X i...\n",
      "--- 🤔: Reflecting on Findings ---\n",
      "  Summary: In 2024, AMD is aggressively competing with NVIDIA in the AI chip market through its Instinct MI300X accelerator, which is being adopted by major cloud providers like Microsoft and Oracle and is projected to generate billions in revenue.\n",
      "--- 🚦: Evaluating Policy ---\n",
      "  -> Plan complete. Finishing.\n",
      "--- ✅: Generating Final Answer with Citations ---\n",
      "\n",
      "--- Graph Stream Finished ---\n"
     ]
    }
   ],
   "source": [
    "final_state = None\n",
    "graph_input = {\"original_question\": complex_query_adv}\n",
    "\n",
    "print(\"--- Invoking Deep Thinking RAG Graph ---\")\n",
    "for chunk in deep_thinking_rag_graph.stream(graph_input, stream_mode=\"values\"):\n",
    "    final_state = chunk\n",
    "\n",
    "print(\"\\n--- Graph Stream Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-2-analyze-pro-adv",
   "metadata": {},
   "source": [
    "### 5.2. Analyzing the Final High-Quality Output with Full Provenance\n",
    "\n",
    "The agent has successfully executed its plan, using the right tool for each step. Now, we examine the `final_answer` stored in the terminal state. Unlike the baseline's failure, we expect a cohesive, multi-part answer that successfully synthesizes information from two different sources into a single analytical response, complete with citations to both the 10-K and the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part5-2-code-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DEEP THINKING RAG FINAL ANSWER ---\n",
      "Based on an analysis of NVIDIA's 2023 10-K filing and recent news from 2024 regarding AMD's AI chip strategy, the following synthesis can be made:\n",
      "\n",
      "**NVIDIA's Stated Competitive Risks:**\n",
      "\n",
      "In its 2023 10-K filing, NVIDIA identifies its operating environment as the intensely competitive semiconductor industry, which is characterized by rapid technological change and evolving standards. A primary risk is that competitors, including AMD, could announce or introduce new products with better performance, features, or lower costs that gain significant market acceptance. The company's success is contingent on its ability to continually innovate and adapt in an industry with short product life cycles, and a failure to do so could materially and adversely affect its business [Source: Item 1A. Risk Factors.].\n",
      "\n",
      "**AMD's 2024 AI Chip Strategy:**\n",
      "\n",
      "In 2024, AMD has moved aggressively to challenge NVIDIA's dominance in the AI hardware market with its Instinct MI300 series of accelerators, particularly the MI300X. This product is designed to compete directly with NVIDIA's H100 GPU for training and inferencing large language models. AMD's strategy has gained significant traction, with major cloud providers such as Microsoft Azure, Meta, and Oracle announcing plans to use the new chips, and the company projects several billion dollars in revenue from these products in 2024 [Source: https://www.reuters.com/technology/amd-forecasts-35-billion-ai-chip-revenue-2024-2024-01-30/].\n",
      "\n",
      "**Synergy and Impact:**\n",
      "\n",
      "AMD's 2024 AI chip strategy directly exacerbates the competitive risks outlined in NVIDIA's 10-K. The successful launch and adoption of the MI300X is a materialization of the specific risk that a competitor could introduce a product with comparable or superior performance. The adoption of AMD's chips by major cloud providers signifies a direct challenge to NVIDIA's market share in the lucrative data center segment. This development validates NVIDIA's stated concerns about rapid technological change and the need for constant innovation to maintain its leadership position, as AMD is now providing a viable, high-performance alternative to NVIDIA's offerings [Source: Item 1A. Risk Factors. and https://www.cnbc.com/2023/12/06/amd-launches-new-mi300x-ai-chip-to-compete-with-nvidias-h100.html].\n"
     ]
    }
   ],
   "source": [
    "console.print(\"--- DEEP THINKING RAG FINAL ANSWER ---\")\n",
    "console.print(Markdown(final_state['final_answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-3-compare-pro-adv",
   "metadata": {},
   "source": [
    "### 5.3. Side-by-Side Comparison: Vanilla RAG vs. Deep Thinking RAG\n",
    "\n",
    "| Feature                 | Vanilla RAG (Failed)                                                                                                                              | Deep Thinking RAG (Success)                                                                                                                                                                                                                                                                                            |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Cognitive Model**     | Linear, stateless, one-shot retrieval.                                                                                                            | Cyclical, stateful, multi-step reasoning loop.                                                                                                                                                                                                                                                                         |\n",
    "| **Planning**            | None. The entire complex query is treated as a single search.                                                                                     | Explicit planning step decomposes the query into a structured, multi-step research plan, **assigning the correct tool (internal vs. web) to each step.**                                                                                                                                                                 |\n",
    "| **Retrieval Strategy**  | Naive semantic search on a single static source.                                                                             | **Adaptive, multi-stage funnel:** A supervisor agent **dynamically selects the best retrieval strategy** (vector, keyword, or hybrid) for each sub-question, followed by a cross-encoder for high-precision reranking.                                                                                                         |\n",
    "| **Knowledge Source**    | Restricted to the single, static 10-K document.                                                                                                   | **Multi-source knowledge:** Can seamlessly access both the static internal document and the live web to gather all necessary evidence.                                                                                                                                                                                           |\n",
    "| **Answer Quality**      | Completely failed to answer the second part of the query due to a lack of information. Unable to perform any synthesis.                                     | Answered all parts of the query comprehensively. **Successfully synthesized information from two different sources** (10-K and web search) into a coherent, analytical narrative with verifiable source citations for both.                                                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-intro-eval-pro",
   "metadata": {},
   "source": [
    "## Part 6: A Production-Grade Evaluation Framework\n",
    "\n",
    "To move from anecdotal success to objective validation, we employ a rigorous, automated evaluation framework. We will use the **RAGAs** (RAG Assessment) library to score both our baseline and advanced pipelines across a suite of metrics designed to quantify the quality and reliability of RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-metrics-pro",
   "metadata": {},
   "source": [
    "### 6.1. Evaluation Metrics Overview\n",
    "**Context Precision & Recall** measure the quality of the retrieved information. Precision is the signal-to-noise ratio, while Recall measures whether all relevant information was found.\n",
    "\n",
    "**Answer Faithfulness** measures whether the answer is grounded in the provided context, preventing hallucination.\n",
    "\n",
    "**Answer Correctness** measures how well the answer addresses the user's query when compared to a 'ground truth' ideal answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-4-ragas-code-pro-adv",
   "metadata": {},
   "source": [
    "### 6.2. Code Dependency: Implementing an Automated Evaluation with RAGAs\n",
    "\n",
    "We construct a `Dataset` object for evaluation. This dataset includes our new multi-source user query, the answers generated by both pipelines, their respective retrieved contexts, and a manually crafted 'ground truth' answer. RAGAs then uses LLMs to score our key metrics, providing a quantitative measure of the advanced agent's superiority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part6-4-code-impl-pro-adv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation dataset...\n",
      "Running RAGAs evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "text/plain": [
       "ร่ำ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete.\n",
      "\n",
      "--- RAGAs Evaluation Results ---\n",
      "                     baseline_rag  deep_thinking_rag\n",
      "context_precision        0.500000           1.000000\n",
      "context_recall           0.333333           1.000000\n",
      "faithfulness             1.000000           1.000000\n",
      "answer_correctness       0.395112           0.991458\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Preparing evaluation dataset...\")\n",
    "ground_truth_answer_adv = \"NVIDIA's 2023 10-K lists intense competition and rapid technological change as key risks. This risk is exacerbated by AMD's 2024 strategy, specifically the launch of the MI300X AI accelerator, which directly competes with NVIDIA's H100 and has been adopted by major cloud providers, threatening NVIDIA's market share in the data center segment.\"\n",
    "\n",
    "# Retrieve context for the baseline model for the new query\n",
    "retrieved_docs_for_baseline_adv = baseline_retriever.invoke(complex_query_adv)\n",
    "baseline_contexts = [[doc.page_content for doc in retrieved_docs_for_baseline_adv]]\n",
    "\n",
    "# Consolidate all retrieved documents from all steps for the advanced agent\n",
    "advanced_contexts_flat = []\n",
    "for step in final_state['past_steps']:\n",
    "    advanced_contexts_flat.extend([doc.page_content for doc in step['retrieved_docs']])\n",
    "advanced_contexts = [list(set(advanced_contexts_flat))] # Use set to remove duplicates for a cleaner eval\n",
    "\n",
    "eval_data = {\n",
    "    'question': [complex_query_adv, complex_query_adv],\n",
    "    'answer': [baseline_result, final_state['final_answer']],\n",
    "    'contexts': baseline_contexts + advanced_contexts,\n",
    "    'ground_truth': [ground_truth_answer_adv, ground_truth_answer_adv]\n",
    "}\n",
    "eval_dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "]\n",
    "\n",
    "print(\"Running RAGAs evaluation...\")\n",
    "result = evaluate(eval_dataset, metrics=metrics, is_async=False)\n",
    "print(\"Evaluation complete.\")\n",
    "\n",
    "results_df = result.to_pandas()\n",
    "results_df.index = ['baseline_rag', 'deep_thinking_rag']\n",
    "print(\"\\n--- RAGAs Evaluation Results ---\")\n",
    "print(results_df[['context_precision', 'context_recall', 'faithfulness', 'answer_correctness']].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-5-interpret-pro-adv",
   "metadata": {},
   "source": [
    "### 6.3. Interpreting the Evaluation Scores for Our Advanced Pipeline\n",
    "\n",
    "The quantitative results provide a definitive verdict on the superiority of the Deep Thinking architecture:\n",
    "\n",
    "-   **Context Precision (0.50 vs 1.00):** The baseline's context was only partially relevant, as it could only retrieve general information about competition without the crucial details on AMD's 2024 strategy. The advanced agent's multi-step, multi-tool retrieval achieved a perfect score.\n",
    "-   **Context Recall (0.33 vs 1.00):** The baseline retriever completely missed the information from the web, resulting in a very low recall score. The advanced agent's planning and tool-use ensured all necessary information from all sources was queried, achieving perfect recall.\n",
    "-   **Faithfulness (1.00 vs 1.00):** Both systems were highly faithful to the context they were given. The baseline correctly stated it didn't have the information, and the advanced agent correctly used the information it found.\n",
    "-   **Answer Correctness (0.40 vs 0.99):** This is the ultimate measure of quality. The baseline's answer was less than 40% correct because it was missing the entire second half of the required analysis. The advanced agent's answer was nearly perfect, demonstrating its ability to perform true synthesis across multiple knowledge sources.\n",
    "\n",
    "**Conclusion:** The evaluation provides objective, quantitative proof that the architectural shift to a cyclical, tool-aware, and adaptive reasoning agent results in a dramatic and measurable improvement in performance on complex, real-world queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-intro-prod-pro",
   "metadata": {},
   "source": [
    "## Part 7: Optimizations and Production Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-1-cache-pro",
   "metadata": {},
   "source": [
    "### 7.1. Optimization 1: Implementing a Cache for Repeated Sub-Queries\n",
    "\n",
    "Our agent makes multiple calls to expensive LLMs (Planner, Rewriter, etc.). In a production environment where users may ask similar questions, caching these calls is essential for performance and cost management. LangChain provides built-in caching that can be easily integrated with our agents.\n",
    "\n",
    "```python\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# To enable caching for all LLM calls in the session\n",
    "set_llm_cache(InMemoryCache())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-2-provenance-pro",
   "metadata": {},
   "source": [
    "### 7.2. Feature 1: Provenance and Citations - Building User Trust\n",
    "\n",
    "Users need to trust the answers our agent provides. A critical feature for production is **provenance**. We have implemented this in our `final_answer_node`. By explicitly prompting the final LLM to use the source metadata (`section` title or `URL`) attached to each piece of evidence, we generate citations directly in the final answer. This makes the agent's reasoning transparent and verifiable across all its knowledge sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-3-discussion-pro",
   "metadata": {},
   "source": [
    "### 7.3. Discussion: The Next Level - MDPs and Learned Policies (The DeepRAG Paper)\n",
    "\n",
    "Currently, our Policy and Supervisor Agents use a powerful, general-purpose LLM to make decisions. While highly effective, this can be slow and costly. The academic frontier, as explored in papers like DeepRAG, frames this reasoning process as a **Markov Decision Process (MDP)**. By logging thousands of successful and unsuccessful reasoning traces from our LangSmith project, we could use reinforcement learning to train smaller, specialized 'policy models'. A learned policy could make the `CONTINUE`/`FINISH` decision or the `vector`/`keyword` decision much faster and more cheaply than a full GPT-4o call, while being highly optimized for our specific domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-4-failure-pro",
   "metadata": {},
   "source": [
    "### 7.4. Handling Failure: Graceful Exits and Fallbacks When No Answer is Found\n",
    "\n",
    "A production system must be robust to failure. What if a sub-question yields no relevant documents? Our current agent simply logs this and moves on. A more advanced implementation would involve:\n",
    "1.  **Reflection with Failure Recognition:** The reflection agent could be prompted to recognize when context is insufficient and explicitly state that the sub-question could not be answered.\n",
    "2.  **`REVISE_PLAN` Path:** The policy agent could have a third option, `REVISE_PLAN`. This would route the state back to the `plan_node`, but this time with the full history, prompting it to create a new, better plan to overcome the dead end.\n",
    "3.  **Graceful Exit:** If re-planning also fails, the graph should route to a final `no_answer_node` that explicitly informs the user that a confident answer could not be constructed from the available documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-conclusion-pro",
   "metadata": {},
   "source": [
    "## Part 8: Conclusion and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-1-summary-pro-adv",
   "metadata": {},
   "source": [
    "### 8.1. Summary of Our Journey\n",
    "\n",
    "In this notebook, we have undertaken a complete journey from a rudimentary RAG pipeline to a sophisticated autonomous reasoning agent. We began by demonstrating the inherent limitations of a shallow, single-pass architecture on a complex, multi-source query. We then systematically constructed a **Deep Thinking RAG** system, adding layers of intelligence: a tool-aware strategic planner, an adaptive, high-fidelity multi-stage retrieval funnel, external tool augmentation, and a self-critiquing policy engine. By orchestrating this advanced cognitive architecture with LangGraph, we created a system capable of true, multi-source synthesis. Our final, rigorous evaluation with RAGAs provided objective, quantitative proof of its dramatic superiority over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-2-principles-pro-adv",
   "metadata": {},
   "source": [
    "### 8.2. Key Architectural Principles of Advanced RAG Systems\n",
    "\n",
    "1.  **Stateful Cyclical Reasoning:** The fundamental shift is from linear, stateless chains to cyclical, stateful graphs. Intelligence emerges from the ability to iterate, reflect, and refine.\n",
    "2.  **Decomposition is King:** Complex problems must be broken down. An explicit, structured planning step is the most critical element for tackling multi-hop, multi-source queries.\n",
    "3.  **Tool Augmentation for Comprehensive Knowledge:** No single knowledge source is sufficient. Agents must be able to reason about when their internal knowledge is lacking and autonomously select external tools (like web search) to fill the gaps.\n",
    "4.  **Dynamic Strategy Selection:** Rigidity is fragile. Empowering the agent to dynamically adapt its strategies (e.g., choosing a retrieval method) based on the specific task at hand leads to more efficient and accurate results.\n",
    "5.  **Separation of Recall and Precision:** Retrieval is not a single step. A multi-stage funnel that first maximizes recall and then maximizes precision (Reranking) is essential for finding the right evidence.\n",
    "6.  **Explicit Self-Correction:** A dedicated policy or 'judge' component that inspects progress and controls the loop is the key to autonomy and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-3-future-pro-adv",
   "metadata": {},
   "source": [
    "### 8.3. Future Directions and Further Reading\n",
    "\n",
    "This architecture serves as a powerful and extensible template. Future work could include:\n",
    "-   **Multi-Document Analysis:** Extending the agent to answer questions that require synthesizing information across a *corpus* of documents, not just a single one, by adding a preliminary 'document routing' step.\n",
    "-   **Structured Tool Use:** Empowering the agent with tools to query structured databases (e.g., SQL) or financial data APIs, and allowing the planner to generate the necessary code or queries for those tools.\n",
    "-   **Fine-Tuning a Supervisor Model:** Training a smaller, specialized SLM on traces from LangSmith to perform the Retrieval Supervisor's role, leading to significant cost and latency reductions in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}